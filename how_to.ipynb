{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06e4d0f-10ae-4d8e-843b-9d9617c32996",
   "metadata": {},
   "source": [
    "## Projecting the Social Network in 2025\n",
    "\n",
    "The social network was developed using a depricated deep learning framework version called TensorFlow 1.0. This framework has since been replaced by more modern and easy to use offerings, but since our model depends on it, some unique steps must be taken to accurately create social network projections.\n",
    "\n",
    "#### Accessing the Duke Compute Cluster\n",
    "There are multiple ways to access the DCC. First you will need account access which can be given by David. Once you have it, you can log in by using the following command in your terminal\n",
    "\n",
    "`ssh {netID}@dcc-login.oit.duke.edu`\n",
    "\n",
    "In the DCC you have 3 primary file locations of interest:\n",
    "\n",
    "- /hpc/home/{netID} - Permanent storage location for code / results with 10-20GB of storage.\n",
    "- /work/{netID} - a temporary directory with fast access to data with unlimited storage. Warning, un-changed files will be purged every 90 days. You may need to create this the first time with `mkdir /work/{netID}` the first time you login.\n",
    "- /datacommons/carlsonlab/{netID} - permanent slow storage that our lab pays for. You may also need to create this with `mkdir /datacommons/carlsonlab/{netID}` the first time you login.\n",
    "\n",
    "Lastly, in many cases we want to use new coding environments for different projects. To make this easier, the dcc has created a web portal for running and editing code: http://dcc-ondemand-01.oit.duke.edu\n",
    "\n",
    "Here, you can create a new session using the Jupyter Lab Apptainer Option. The parameters you want to use are:\n",
    "- Account: Carlsonlab\n",
    "- Partition: carlsonlab-gpu\n",
    "- Walltime: 24 hrs (max)\n",
    "- CPUs: 16 (more can be available)\n",
    "- Memory: 80GB (max)\n",
    "- GPUs: 1\n",
    "- Apptainer Container File: /work/{yourNetID}/{containerName}\n",
    "\n",
    "You can copy containers from the datacommons to use for this setup. From here, the program will land you in the home directory where you can create jupyter notebooks and run code. I recommend you store all of your data in the /work/ directory since storage in home is limited.\n",
    "\n",
    "The exact container needed for social network projections is outlined below.\n",
    "\n",
    "#### Setting Up the Environment\n",
    "\n",
    "To run Tensorflow 1.0 code, we will need a python environment that can run it. To do this, copy the following container on the duke compute cluster to your work directory by running the following script once you are already in the dcc.\n",
    "\n",
    "`cp /datacommons/carlsonlab/Containers/social_proj.simg /work/{yourNetID}/`\n",
    "\n",
    "#### Preparing the Data\n",
    "\n",
    "All of the data used for the social paper was prepared using the lpne-data-analysis (https://github.com/carlson-lab/lpne-data-analysis) pipeline. Therefore, feature extraction must be done using this code base before any projections can be done. \n",
    "\n",
    "To do this, you first will need an excel file named channel_info.xlsx or something similar. Column A is a list of all of the recorded electrode names as they appear in the .mat files you wish to process. Column B is the corresponding area name you would like a particular electrode to be averaged into. For example in column A we may see [BLA_1,BLA_2,CeA_1,CeA_2] and in column B we may see [Amy, Amy, Amy, Amy].\n",
    "\n",
    "Second, you need all of your lfp .mat files in a Data folder, and all of the CHANS files in a CHANS folder. The repository goes into more detail about setting this up\n",
    "\n",
    "It is crucial that the column B naming scheme be consistent across all projects!!!!!!!\n",
    "\n",
    "The naming scheme for the social project is:\n",
    "['Amy', 'Cg Cx', 'Hipp', 'IL Cx', 'Nac', 'PrL Cx', 'Thal', 'VTA']\n",
    "\n",
    "Do not mixup any ordering of Cg_Cx for Cx_Cg or anything similar since features will be computed and sorted alphabetically and your projections will not be valid.\n",
    "\n",
    "Additionally, you must save your features using the \"saveFeatures_1.1\" version. This is what matches the original work.\n",
    "\n",
    "From here, you can follow the demos provided in the lpne-data-analysis repository. Or if it is more convenient, here is a matlab script you can adapt to your dataset:\n",
    "\n",
    "```\n",
    "chan_info_file = \"{Your Channel Info File Location}\";\n",
    "base_dir = \"{Location of your Data and CHANS subdirs parent directory}\";\n",
    "\n",
    "secs_per_window = 1;\n",
    "sample_rate = 1000;\n",
    "\n",
    "opts.mvgcFolder = './mvgc/';\n",
    "opts.parCores = 32;\n",
    "opts.version.power = 'saveFeatures_1.1';\n",
    "opts.version.coherence = 'saveFeatures_1.1';\n",
    "opts.version.granger = 'saveFeatures_1.1';\n",
    "opts.featureList = {'power','coherence','granger'};\n",
    "\n",
    "savePath = base_dir + \"{name of your saved features}.mat\";\n",
    "formatWindows(char(savePath),false,char(base_dir),char(chan_info_file),sample_rate,secs_per_window)\n",
    "preprocessData(char(savePath))\n",
    "saveFeatures(char(savePath),opts)\n",
    "```\n",
    "\n",
    "### After feature generation\n",
    "\n",
    "There are no automated scripts for aligning behavioral labels. Most of the time, this is easiest to accomplish using the time labels from the features.\n",
    "\n",
    "Once you have your features and your labels, AND YOU ARE CERTAIN THE FEATURE VERSION (1.1) AND THE BRAIN REGION AREA NAMES ARE CORRECT, we are ready to project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7b68ad-1773-4316-81b4-ae57cafc9219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /hpc/home/mk423/Social_Network_Projections/norm_base.py:40: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.decomposition as dp\n",
    "import pickle\n",
    "import sys,os\n",
    "import numpy.random as rand\n",
    "from norm_encoded import NMF \n",
    "from norm_supervised import sNMF\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import auc,roc_curve\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "import scipy.io\n",
    "from data_tools import load_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64295c-267f-4b07-a3af-2b35fe768415",
   "metadata": {},
   "source": [
    "### Import your Data\n",
    "\n",
    "Data is imported using the data_tools library. In preprocessing, granger features are exponentiated to make them more linear and then are cut-off at magnitudes greater than 10 as is done in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762402ef-e000-4809-ae60-a9f2490575ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"/work/mk423/Social_v_1_2/Social_Chunk_1_v_1_1.mat\"\n",
    "\n",
    "#Actually load in the data using the correct contemporary loader\n",
    "myList = load_data(fileName,fBounds=(1,56),feature_list=['power','coherence','granger'])\n",
    "\n",
    "#Gets the power, coherence and exp(granger) features along with truncation\n",
    "power = myList[0]\n",
    "coherence = myList[1]\n",
    "granger = myList[2]\n",
    "granger = np.exp(granger)\n",
    "granger[granger>10] = 10\n",
    "\n",
    "labels = myList[3]\n",
    "windows = labels['windows']\n",
    "mouse = windows['mouse']\n",
    "time = windows['time']\n",
    "expDate = windows['expDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a26771-1881-4057-9072-7cad506a5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When stacking the features together, the power features are multiplied by 10. This is done for the feature\n",
    "#magnitudes to match more closely\n",
    "X = np.hstack([10*power,coherence,granger])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba5f9d-5581-4764-addc-55f6c18891d2",
   "metadata": {},
   "source": [
    "### Construct the Model\n",
    "\n",
    "These parameters and initializations were defined by austin in his original projection demo. They retrieve the original model parameters from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e97f1a4-2ddc-4dd8-bb9b-7c74c84bd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter=5\n",
    "dev=3\n",
    "number_test = str(3906)\n",
    "number_components = str(9)\n",
    "trial = str(6)\n",
    "dirName = './supervised_rep_' + number_test+'_'+number_components+'_'+trial\n",
    "supStr=3.0\n",
    "model = sNMF(5,outerIter=nIter,device=dev,dirName=dirName,LR=1e-5,\n",
    "                percGPU=.35,\n",
    "                n_blessed=1,mu=supStr)\n",
    "\n",
    "#Actually load mapping from subdirectory\n",
    "model.meta = model.dirName + '/' + model.name + '.ckpt.meta'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae34a47-1957-41e0-9401-128819c9cc57",
   "metadata": {},
   "source": [
    "### Actually Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff65b7-d200-418f-8d75-2d8d74bd29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The projection\n",
    "S_DEP = model.transform(X)\n",
    "\n",
    "s_social = S_DEP[:,0]\n",
    "s_unsup = S_DEP[:,1:]\n",
    "\n",
    "#save relevant variables to a csv\n",
    "saveDict = {\n",
    "    \"s_social\":s_social,\n",
    "    \"time\":time,\n",
    "    \"mouse\":mouse,\n",
    "    \"expDate\":expDate,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(saveDict)\n",
    "#df.to_csv(\"demo.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdcd0b-f385-46ca-a905-8a6183d55bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
